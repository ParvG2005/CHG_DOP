""
"#
Contextual Head Gating(CHG) - Quick Demo

This notebook walks through:
    1. Loading a model and installing gates
2. Fitting gate parameters
3. Running ablation experiments
4. Visualizing results

Run this to quickly understand how CHG works!
    ""
"

# % % Cell 1: Setup
import sys
sys.path.insert(0, '..')

import torch
from transformers
import AutoModelForCausalLM, AutoTokenizer
from src.model.gate_wrapper
import HeadGates, install_attention_hooks, remove_hooks
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Check device
device = 'cuda'
if torch.cuda.is_available()
else 'cpu'
print(f "Using device: {device}")

# % % Cell 2: Load Model
print("Loading GPT-2...")
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
model.eval()

# Get model dimensions
num_layers = model.config.num_hidden_layers# 12
for GPT - 2
num_heads = model.config.num_attention_heads# 12
for GPT - 2

print(f "Model: {num_layers} layers, {num_heads} heads per layer")

# % % Cell 3: Initialize Gates
print("Initializing gates...")
gates = HeadGates(num_layers, num_heads, init_value = 2.0)
gates.to(device)

# Check initial gate values
with torch.no_grad():
    initial_gates = gates.get_gates()
print(f "Initial gate values: {initial_gates[0, :5]}...")# First 5 heads of layer 0
print(f "Mean: {initial_gates.mean():.3f}, Std: {initial_gates.std():.3f}")

# % % Cell 4: Install Hooks
print("Installing attention hooks...")
handles = install_attention_hooks(model, gates)
print(f "Installed {len(handles)} hooks")

# % % Cell 5: Test Forward Pass
print("\nTesting forward pass with gates...")
test_prompt = "The capital of France is"
inputs = tokenizer(test_prompt, return_tensors = 'pt').to(device)

with torch.no_grad():
    outputs = model( ** inputs)
baseline_logprob = -outputs.loss.item() if hasattr(outputs, 'loss')
else 0

print(f "Baseline log-prob: {baseline_logprob:.4f}")

# % % Cell 6: Ablate a Single Head
print("\nAblating a single head...")# Save original gate value
original_value = gates.logits[0, 0].item()

# Ablate head 0 of layer 0
gates.ablate_head(0, 0, value = 0.0)

with torch.no_grad():
    outputs = model( ** inputs)
ablated_logprob = -outputs.loss.item() if hasattr(outputs, 'loss')
else 0

print(f "Ablated log-prob: {ablated_logprob:.4f}")
print(f "Change: {ablated_logprob - baseline_logprob:.4f}")

# Restore
gates.logits.data[0, 0] = original_value

# % % Cell 7: Compute All Head Effects
print("\nComputing individual head effects...")
print("This may take a few minutes...")

effects = np.zeros((num_layers, num_heads))
saved_gates = gates.logits.data.clone()

for layer_idx in range(num_layers):
    print(f "Layer {layer_idx+1}/{num_layers}...", end = ' ')
for head_idx in range(num_heads): #Reset gates
gates.logits.data = saved_gates.clone()

# Ablate this head
gates.ablate_head(layer_idx, head_idx, value = 0.0)

# Evaluate
with torch.no_grad():
    outputs = model( ** inputs)
logprob = -outputs.loss.item() if hasattr(outputs, 'loss')
else 0

effects[layer_idx, head_idx] = logprob - baseline_logprob
print("âœ“")

# Restore gates
gates.logits.data = saved_gates

print("\nHead effect statistics:")
print(f "  Mean: {effects.mean():.4f}")
print(f "  Std: {effects.std():.4f}")
print(f "  Min (most important): {effects.min():.4f}")
print(f "  Max (least important): {effects.max():.4f}")

# % % Cell 8: Visualize Results
print("\nGenerating visualizations...")

fig, axes = plt.subplots(1, 3, figsize = (18, 5))

# Heatmap of all heads
sns.heatmap(effects, cmap = 'RdYlGn_r', center = 0,
    annot = True, fmt = '.3f', cbar_kws = { 'label': 'Log-Prob Change' },
    ax = axes[0])
axes[0].set_title('Head Importance by Layer and Position', fontweight = 'bold')
axes[0].set_xlabel('Head Index')
axes[0].set_ylabel('Layer Index')

# Layer - wise average
layer_avg = effects.mean(axis = 1)
axes[1].bar(range(num_layers), layer_avg, color = '#2E86AB', alpha = 0.7, edgecolor = 'black')
axes[1].axhline(y = 0, color = 'red', linestyle = '--', alpha = 0.5)
axes[1].set_xlabel('Layer Index')
axes[1].set_ylabel('Average Importance')
axes[1].set_title('Average Head Importance by Layer', fontweight = 'bold')
axes[1].grid(True, alpha = 0.3, axis = 'y')

# Distribution of effects
axes[2].hist(effects.flatten(), bins = 30, color = '#A23B72', alpha = 0.7, edgecolor = 'black')
axes[2].axvline(x = 0, color = 'red', linestyle = '--', label = 'Zero effect')
axes[2].set_xlabel('Log-Prob Change')
axes[2].set_ylabel('Frequency')
axes[2].set_title('Distribution of Head Effects', fontweight = 'bold')
axes[2].legend()
axes[2].grid(True, alpha = 0.3, axis = 'y')

plt.tight_layout()
plt.savefig('demo_results.png', dpi = 150, bbox_inches = 'tight')
print("Saved visualization to demo_results.png")
plt.show()

# % % Cell 9: Find Most / Least Important Heads
print("\nTop 5 Most Important Heads:")
flat_effects = effects.flatten()
flat_indices = np.argsort(flat_effects)

for i, idx in enumerate(flat_indices[: 5]):
    layer = idx // num_heads
head = idx % num_heads
effect = flat_effects[idx]
print(f "  {i+1}. Layer {layer}, Head {head}: {effect:.4f}")

print("\nTop 5 Least Important Heads:")
for i, idx in enumerate(flat_indices[-5: ]):
    layer = idx // num_heads
head = idx % num_heads
effect = flat_effects[idx]
print(f "  {i+1}. Layer {layer}, Head {head}: {effect:.4f}")

# % % Cell 10: Cleanup
print("\nCleaning up...")
remove_hooks(handles)
print("Removed hooks")

# % % Cell 11: Optional - Fit Gates with Regularization
print("\nOptional: Fitting gates with regularization...")
print("(This demonstrates the training loop)")

# Create fresh gates
training_gates = HeadGates(num_layers, num_heads, init_value = 2.0).to(device)
training_handles = install_attention_hooks(model, training_gates)

# Setup optimizer
optimizer = torch.optim.AdamW([training_gates.logits], lr = 0.01)

# Quick training loop(just 5 steps
    for demo)
lambda_reg = -0.1# Negative = retention(encourage high gates)

print("Training for 5 steps...")
for step in range(5): #Forward pass
outputs = model( ** inputs)
loss_nll = outputs.loss
if hasattr(outputs, 'loss')
else torch.tensor(0.0)

# Regularization: penalize low gates
g = torch.sigmoid(training_gates.logits)
loss_reg = -lambda_reg * torch.log(g.clamp(1e-7, 1 - 1e-7)).sum()

# Total loss
loss = loss_nll + loss_reg

# Backward
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f "  Step {step+1}: Loss={loss.item():.4f}, NLL={loss_nll.item():.4f}, Reg={loss_reg.item():.4f}")

# Check how gates changed
with torch.no_grad():
    trained_gates = training_gates.get_gates()
print(f "\nAfter training:")
print(f "  Mean gate: {trained_gates.mean():.3f} (was {initial_gates.mean():.3f})")
print(f "  Std gate: {trained_gates.std():.3f} (was {initial_gates.std():.3f})")

remove_hooks(training_handles)

# % % Cell 12: Save Results
print("\nSaving demo results...")

# Save effects
np.save('demo_head_effects.npy', effects)

# Save gates
training_gates.save('demo_gates.pt')

print("Saved:")
print("  - demo_head_effects.npy")
print("  - demo_gates.pt")
print("  - demo_results.png")

# % % Cell 13: Summary
print("\n" + "=" * 60)
print("DEMO COMPLETE!")
print("=" * 60)
print("\nYou've learned:")
print("  âœ“ How to initialize and install gates")
print("  âœ“ How to ablate individual heads")
print("  âœ“ How to compute head importance scores")
print("  âœ“ How to visualize results")
print("  âœ“ How to train gates with regularization")
print("\nNext steps:")
print("  1. Try with different models (gpt2-medium, gpt2-large)")
print("  2. Test on different prompts or datasets")
print("  3. Run full training pipeline (see README)")
print("  4. Experiment with contrastive CHG")
print("\nHappy analyzing! ðŸŽ‰")